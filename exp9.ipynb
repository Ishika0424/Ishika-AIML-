{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvngG3h83R2P"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Dataset\n",
        "df = pd.read_csv(\"imbalanced_dataset.csv\")  # Replace with your dataset path\n",
        "print(df.info())\n",
        "print(df['target'].value_counts())  # Replace 'target' with the target column name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9k5g3RY361j",
        "outputId": "55aa987a-71b8-4ce7-b484-38206bc6708a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 11 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Feature_1   1000 non-null   float64\n",
            " 1   Feature_2   1000 non-null   float64\n",
            " 2   Feature_3   1000 non-null   float64\n",
            " 3   Feature_4   1000 non-null   float64\n",
            " 4   Feature_5   1000 non-null   float64\n",
            " 5   Feature_6   1000 non-null   float64\n",
            " 6   Feature_7   1000 non-null   float64\n",
            " 7   Feature_8   1000 non-null   float64\n",
            " 8   Feature_9   1000 non-null   float64\n",
            " 9   Feature_10  1000 non-null   float64\n",
            " 10  target      1000 non-null   int64  \n",
            "dtypes: float64(10), int64(1)\n",
            "memory usage: 86.1 KB\n",
            "None\n",
            "target\n",
            "0    896\n",
            "1    104\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "X = df.drop(columns='target')  # Replace 'target' with your column\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Display the shapes of the train-test splits\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFAaJLq_4Dow",
        "outputId": "03c5dc0a-7460-4822-b50f-04e8702f378e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (700, 10)\n",
            "X_test shape: (300, 10)\n",
            "y_train shape: (700,)\n",
            "y_test shape: (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('imbalanced_dataset.csv')\n",
        "\n",
        "# Prepare the features and target\n",
        "X = df.drop(columns='target')\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_model(X_train, y_train, X_test, y_test, method):\n",
        "    # Create a Logistic Regression model\n",
        "    model = LogisticRegression(class_weight='balanced' if method == 'Class Weighting' else None, max_iter=1000)\n",
        "\n",
        "    try:\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = precision_score(y_test, y_pred, average=\"binary\")\n",
        "        recall = recall_score(y_test, y_pred, average=\"binary\")\n",
        "        f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
        "        auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"--- {method} ---\")\n",
        "        print(f\"Precision: {precision:.2f}\")\n",
        "        print(f\"Recall: {recall:.2f}\")\n",
        "        print(f\"F1-Score: {f1:.2f}\")\n",
        "        print(f\"AUC-ROC: {auc:.2f}\")\n",
        "\n",
        "        return [method, precision, recall, f1, auc]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {method}: {str(e)}\")\n",
        "        return [method, None, None, None, None]\n",
        "\n",
        "# Apply techniques and evaluate\n",
        "\n",
        "# a) Random Oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "evaluate_model(X_resampled, y_resampled, X_test, y_test, 'Random Oversampling')\n",
        "\n",
        "# b) Random Undersampling\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "evaluate_model(X_resampled, y_resampled, X_test, y_test, 'Random Undersampling')\n",
        "\n",
        "# c) SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "evaluate_model(X_resampled, y_resampled, X_test, y_test, 'SMOTE')\n",
        "\n",
        "# d) Class Weighting\n",
        "evaluate_model(X_train, y_train, X_test, y_test, 'Class Weighting')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmcvogWg36yA",
        "outputId": "7c99ae40-3c21-47d5-fbad-3bc043798a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Random Oversampling ---\n",
            "Precision: 0.32\n",
            "Recall: 0.75\n",
            "F1-Score: 0.45\n",
            "AUC-ROC: 0.78\n",
            "--- Random Undersampling ---\n",
            "Precision: 0.31\n",
            "Recall: 0.84\n",
            "F1-Score: 0.46\n",
            "AUC-ROC: 0.81\n",
            "--- SMOTE ---\n",
            "Precision: 0.32\n",
            "Recall: 0.75\n",
            "F1-Score: 0.45\n",
            "AUC-ROC: 0.78\n",
            "--- Class Weighting ---\n",
            "Precision: 0.32\n",
            "Recall: 0.75\n",
            "F1-Score: 0.45\n",
            "AUC-ROC: 0.78\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Class Weighting', 0.32, 0.75, 0.4485981308411215, 0.7798507462686566]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('imbalanced_dataset.csv')\n",
        "\n",
        "# Prepare the features and target\n",
        "X = df.drop(columns='target')\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_model(X_train, y_train, X_test, y_test, method):\n",
        "    print(f\"Evaluating with {method}...\")\n",
        "    model = LogisticRegression(class_weight='balanced' if method == 'Class Weighting' else None, max_iter=1000)\n",
        "\n",
        "    try:\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = precision_score(y_test, y_pred, average=\"binary\")\n",
        "        recall = recall_score(y_test, y_pred, average=\"binary\")\n",
        "        f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
        "        auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"--- {method} ---\")\n",
        "        print(f\"Precision: {precision:.2f}\")\n",
        "        print(f\"Recall: {recall:.2f}\")\n",
        "        print(f\"F1-Score: {f1:.2f}\")\n",
        "        print(f\"AUC-ROC: {auc:.2f}\")\n",
        "\n",
        "        # Return the results to be stored in the list\n",
        "        return [method, precision, recall, f1, auc]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {method}: {str(e)}\")\n",
        "        return [method, None, None, None, None]\n",
        "\n",
        "# Initialize the list to store the results\n",
        "results = []\n",
        "\n",
        "# Apply techniques and evaluate\n",
        "\n",
        "# a) Random Oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "results.append(evaluate_model(X_resampled, y_resampled, X_test, y_test, 'Random Oversampling'))\n",
        "\n",
        "# b) Random Undersampling\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "results.append(evaluate_model(X_resampled, y_resampled, X_test, y_test, 'Random Undersampling'))\n",
        "\n",
        "# c) SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "results.append(evaluate_model(X_resampled, y_resampled, X_test, y_test, 'SMOTE'))\n",
        "\n",
        "# d) Class Weighting\n",
        "results.append(evaluate_model(X_train, y_train, X_test, y_test, 'Class Weighting'))\n",
        "\n",
        "# Summarize Results in a DataFrame\n",
        "results_df = pd.DataFrame(results, columns=[\"Method\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"])\n",
        "\n",
        "# Display the summary of results\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Ja5Dlm36uC",
        "outputId": "2fe13d34-54bc-4a1f-dc9b-d9fbdc195474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating with Random Oversampling...\n",
            "--- Random Oversampling ---\n",
            "Precision: 0.32\n",
            "Recall: 0.75\n",
            "F1-Score: 0.45\n",
            "AUC-ROC: 0.78\n",
            "Evaluating with Random Undersampling...\n",
            "--- Random Undersampling ---\n",
            "Precision: 0.31\n",
            "Recall: 0.84\n",
            "F1-Score: 0.46\n",
            "AUC-ROC: 0.81\n",
            "Evaluating with SMOTE...\n",
            "--- SMOTE ---\n",
            "Precision: 0.32\n",
            "Recall: 0.75\n",
            "F1-Score: 0.45\n",
            "AUC-ROC: 0.78\n",
            "Evaluating with Class Weighting...\n",
            "--- Class Weighting ---\n",
            "Precision: 0.32\n",
            "Recall: 0.75\n",
            "F1-Score: 0.45\n",
            "AUC-ROC: 0.78\n",
            "\n",
            "Summary of Results:\n",
            "                 Method  Precision   Recall  F1-Score   AUC-ROC\n",
            "0   Random Oversampling   0.320000  0.75000  0.448598  0.779851\n",
            "1  Random Undersampling   0.313953  0.84375  0.457627  0.811800\n",
            "2                 SMOTE   0.324324  0.75000  0.452830  0.781716\n",
            "3       Class Weighting   0.320000  0.75000  0.448598  0.779851\n"
          ]
        }
      ]
    }
  ]
}